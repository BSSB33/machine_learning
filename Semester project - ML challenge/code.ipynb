{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd02c829f9b014f45ac51ac6981ee268f80d78860744c6a50c09e5b990310b7d8d4",
   "display_name": "Python 3.8.8 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "2c829f9b014f45ac51ac6981ee268f80d78860744c6a50c09e5b990310b7d8d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest,chi2,RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      C1   C2   C3  C4  C5   C6   C7   C8  C9 C10  ...    X11    X12    X13  \\\n",
       "0     V1   V1   V1  V1  V1   V1   V1   V1  V1  V1  ... -0.619 -0.536 -0.092   \n",
       "1     V1   V1   V1  V1  V1   V1   V2   V1  V3  V5  ...  0.352  0.073 -0.092   \n",
       "2     V1   V1   V1  V1  V1   V1   V1   V1  V2  V2  ... -1.933 -0.536 -0.092   \n",
       "3     V1   V1   V1  V1  V1   V1   V1   V1  V2  V2  ... -0.762 -0.536 -0.092   \n",
       "4     V1   V1   V1  V1  V1   V1   V1   V1  V1  V1  ... -0.505 -0.536 -0.092   \n",
       "..   ...  ...  ...  ..  ..  ...  ...  ...  ..  ..  ...    ...    ...    ...   \n",
       "395  NaN  NaN  NaN  V3  V1  NaN  NaN  NaN  V1  V4  ...    NaN    NaN    NaN   \n",
       "396  NaN  NaN  NaN  V3  V2  NaN  NaN   V1  V3  V2  ...    NaN    NaN    NaN   \n",
       "397  NaN  NaN  NaN  V3  V2  NaN  NaN  NaN  V1  V4  ...    NaN    NaN    NaN   \n",
       "398  NaN  NaN  NaN  V3  V1  NaN  NaN  NaN  V1  V8  ...    NaN    NaN    NaN   \n",
       "399  NaN  NaN  NaN  V3  V1  NaN  NaN   V1  V3  V2  ...    NaN    NaN    NaN   \n",
       "\n",
       "       X14    X15    X16    X17    X18    X19  Y  \n",
       "0    0.182  0.034 -0.172  0.401  0.393  0.216  1  \n",
       "1    1.098  0.034  1.160  0.401  0.037  0.216  1  \n",
       "2    1.098  0.034  0.716  0.401  0.724  0.216 -1  \n",
       "3    1.098  0.034  0.716  0.401  0.712  0.216 -1  \n",
       "4    0.182  0.034  0.716  0.401  0.393  0.216  1  \n",
       "..     ...    ...    ...    ...    ...    ... ..  \n",
       "395 -2.568  0.034    NaN  0.401  1.890  0.216  1  \n",
       "396 -4.401 -2.684    NaN  0.401  1.391  0.216  1  \n",
       "397  0.548  4.926    NaN  0.401  0.992  0.216  1  \n",
       "398 -1.651  3.567    NaN  0.401  0.992  0.216  1  \n",
       "399  0.182  0.034    NaN  0.401  1.012  0.216  1  \n",
       "\n",
       "[400 rows x 34 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>C1</th>\n      <th>C2</th>\n      <th>C3</th>\n      <th>C4</th>\n      <th>C5</th>\n      <th>C6</th>\n      <th>C7</th>\n      <th>C8</th>\n      <th>C9</th>\n      <th>C10</th>\n      <th>...</th>\n      <th>X11</th>\n      <th>X12</th>\n      <th>X13</th>\n      <th>X14</th>\n      <th>X15</th>\n      <th>X16</th>\n      <th>X17</th>\n      <th>X18</th>\n      <th>X19</th>\n      <th>Y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>...</td>\n      <td>-0.619</td>\n      <td>-0.536</td>\n      <td>-0.092</td>\n      <td>0.182</td>\n      <td>0.034</td>\n      <td>-0.172</td>\n      <td>0.401</td>\n      <td>0.393</td>\n      <td>0.216</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V2</td>\n      <td>V1</td>\n      <td>V3</td>\n      <td>V5</td>\n      <td>...</td>\n      <td>0.352</td>\n      <td>0.073</td>\n      <td>-0.092</td>\n      <td>1.098</td>\n      <td>0.034</td>\n      <td>1.160</td>\n      <td>0.401</td>\n      <td>0.037</td>\n      <td>0.216</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V2</td>\n      <td>V2</td>\n      <td>...</td>\n      <td>-1.933</td>\n      <td>-0.536</td>\n      <td>-0.092</td>\n      <td>1.098</td>\n      <td>0.034</td>\n      <td>0.716</td>\n      <td>0.401</td>\n      <td>0.724</td>\n      <td>0.216</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V2</td>\n      <td>V2</td>\n      <td>...</td>\n      <td>-0.762</td>\n      <td>-0.536</td>\n      <td>-0.092</td>\n      <td>1.098</td>\n      <td>0.034</td>\n      <td>0.716</td>\n      <td>0.401</td>\n      <td>0.712</td>\n      <td>0.216</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>...</td>\n      <td>-0.505</td>\n      <td>-0.536</td>\n      <td>-0.092</td>\n      <td>0.182</td>\n      <td>0.034</td>\n      <td>0.716</td>\n      <td>0.401</td>\n      <td>0.393</td>\n      <td>0.216</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>395</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>V3</td>\n      <td>V1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>V1</td>\n      <td>V4</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-2.568</td>\n      <td>0.034</td>\n      <td>NaN</td>\n      <td>0.401</td>\n      <td>1.890</td>\n      <td>0.216</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>396</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>V3</td>\n      <td>V2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>V1</td>\n      <td>V3</td>\n      <td>V2</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-4.401</td>\n      <td>-2.684</td>\n      <td>NaN</td>\n      <td>0.401</td>\n      <td>1.391</td>\n      <td>0.216</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>397</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>V3</td>\n      <td>V2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>V1</td>\n      <td>V4</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.548</td>\n      <td>4.926</td>\n      <td>NaN</td>\n      <td>0.401</td>\n      <td>0.992</td>\n      <td>0.216</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>398</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>V3</td>\n      <td>V1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>V1</td>\n      <td>V8</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-1.651</td>\n      <td>3.567</td>\n      <td>NaN</td>\n      <td>0.401</td>\n      <td>0.992</td>\n      <td>0.216</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>399</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>V3</td>\n      <td>V1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>V1</td>\n      <td>V3</td>\n      <td>V2</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.182</td>\n      <td>0.034</td>\n      <td>NaN</td>\n      <td>0.401</td>\n      <td>1.012</td>\n      <td>0.216</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>400 rows × 34 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\", header=None)\n",
    "new_cols = ['C' + str(i) for i in range(1, 14 + 1)] + ['X' + str(i) for i in range(1, 19 + 2)]\n",
    "new_cols[-1] = 'Y'\n",
    "df.columns = new_cols\n",
    "\n",
    "features = df.loc[:, df.columns != 'Y']\n",
    "numerical_features = df.select_dtypes(include=['float64']).columns\n",
    "categorical_features = df.select_dtypes(include=['object']).columns\n",
    "target = df['Y']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "68.77637130801688\n"
     ]
    }
   ],
   "source": [
    "print((np.count_nonzero(target == 1)/np.count_nonzero(target == -1)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Gábor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\impute\\_iterative.py:685: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n  warnings.warn(\"[IterativeImputer] Early stopping criterion not\"\nC:\\Users\\Gábor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._setitem_single_column(loc, value[:, i].tolist(), pi)\nC:\\Users\\Gábor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._setitem_single_column(loc, value[:, i].tolist(), pi)\nC:\\Users\\Gábor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._setitem_single_column(loc, value[:, i].tolist(), pi)\nC:\\Users\\Gábor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "#TODO Experiment with strategies\n",
    "iterative_numerical_imputer = IterativeImputer(max_iter=10)\n",
    "iterative_numerical_imputer.fit(X_train[numerical_features])\n",
    "X_train.loc[:, numerical_features] = iterative_numerical_imputer.transform(X_train.loc[:, numerical_features])\n",
    "X_test.loc[:, numerical_features] = iterative_numerical_imputer.transform(X_test.loc[:, numerical_features])\n",
    "\n",
    "categorical_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "categorical_imputer.fit(X_train[categorical_features])\n",
    "X_train.loc[:, categorical_features] = categorical_imputer.transform(X_train.loc[:, categorical_features])\n",
    "X_test.loc[:, categorical_features] = categorical_imputer.transform(X_test.loc[:, categorical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_len = len(X_train)\n",
    "X = pd.concat([X_train, X_test])\n",
    "X = pd.get_dummies(X)\n",
    "X_train = X[:X_train_len]\n",
    "X_test = X[X_train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.to_csv(\"save\\X.csv\", index=False)\n",
    "#X_train.to_csv(\"save\\X_train.csv\", index=False)\n",
    "#X_test.to_csv(\"save\\X_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Score:  0.8\n",
      "Test Score:  0.825\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classifier1 = RandomForestClassifier(n_estimators=1000)\n",
    "classifier1.fit(X_train, y_train)\n",
    "print(\"Test Score: \", classifier1.score(X_test, y_test))\n",
    "\n",
    "classifier2 = ExtraTreesClassifier(n_estimators=1000)\n",
    "classifier2.fit(X_train, y_train)\n",
    "print(\"Test Score: \", classifier2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RandomForestClassifier\n",
      "[[48  3]\n",
      " [13 16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.79      0.94      0.86        51\n",
      "           1       0.84      0.55      0.67        29\n",
      "\n",
      "    accuracy                           0.80        80\n",
      "   macro avg       0.81      0.75      0.76        80\n",
      "weighted avg       0.81      0.80      0.79        80\n",
      "\n",
      "ExtraTreesClassifier\n",
      "[[47  4]\n",
      " [10 19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.82      0.92      0.87        51\n",
      "           1       0.83      0.66      0.73        29\n",
      "\n",
      "    accuracy                           0.82        80\n",
      "   macro avg       0.83      0.79      0.80        80\n",
      "weighted avg       0.83      0.82      0.82        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = classifier1.predict(X_test)\n",
    "cm1 = confusion_matrix(y_test, y_pred1)\n",
    "\n",
    "print(\"RandomForestClassifier\")\n",
    "print(cm1)\n",
    "print(classification_report(y_test, y_pred1))\n",
    "\n",
    "\n",
    "y_pred2 = classifier2.predict(X_test)\n",
    "cm2 = confusion_matrix(y_test, y_pred2)\n",
    "\n",
    "print(\"ExtraTreesClassifier\")\n",
    "print(cm2)\n",
    "print(classification_report(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean Absolute Error: 0.4\nMean Squared Error: 0.8\nRoot Mean Squared Error: 0.8944271909999159\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred1))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred1))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">SVM 0.746 (0.047)\n",
      ">KNN 0.765 (0.050)\n",
      ">BAG 0.801 (0.034)\n",
      "> RF 0.818 (0.028)\n",
      "> ET 0.830 (0.040)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(X_train, y_train, X_test, y_test, model):\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\t#model.fit(X_train, y_train)\n",
    "\treturn cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "models, names = list(), list()\n",
    "# SVM\n",
    "models.append(SVC(gamma='auto'))\n",
    "names.append('SVM')\n",
    "# KNN\n",
    "models.append(KNeighborsClassifier())\n",
    "names.append('KNN')\n",
    "# Neutral Network\n",
    "#models.append(MLPClassifier(solver = 'adam', max_iter=1000, tol=0.000001, early_stopping=True, validation_fraction=0.1, n_iter_no_change=20))\n",
    "#names.append('NEU')\n",
    "# Bagging\n",
    "models.append(BaggingClassifier(n_estimators=1000))\n",
    "names.append('BAG')\n",
    "# RF\n",
    "models.append(RandomForestClassifier(n_estimators=1000))\n",
    "names.append(' RF')\n",
    "# ET\n",
    "models.append(ExtraTreesClassifier(n_estimators=1000))\n",
    "names.append(' ET')\n",
    "\n",
    "\n",
    "results = list()\n",
    "for i in range(len(models)):\n",
    "\tscores = evaluate_model(X_train, y_train, X_test, y_test, models[i])\n",
    "\tresults.append(scores)\n",
    "\tprint('>%s %.3f (%.3f)' % (names[i], scores.mean(), scores.std()))\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'n_estimators': [1000, 1250, 1500, 1750, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': [int(x) for x in np.linspace(start = 1000, stop = 2000, num = 5)],\n",
    "               'max_features': ['auto', 'sqrt'],\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': [2, 5, 10],\n",
    "               'min_samples_leaf': [1, 2, 4],\n",
    "               'bootstrap': [True, False]}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Elapsed time:  141.53899812698364  seconds, which is:  2.358983302116394  minutes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "start = time.time()\n",
    "\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Elapsed time: \", end - start, \" seconds, which is: \", (end - start) / 60, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'n_estimators': 1000,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 90,\n",
       " 'bootstrap': False}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy = 0.82%.\nAccuracy = 0.81%.\nImprovement of -1.52%.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "def evaluate(model, X_test, y_test):\n",
    "    #TODO swap to cross_val\n",
    "    y_pred = model.predict(X_test)\n",
    "    errors = abs(y_pred - y_test)\n",
    "    mape = 100 * np.mean(errors / y_test)\n",
    "    accuracy = 100 - mape\n",
    "    print('\\nModel Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    #print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return accuracy\n",
    "\"\"\"\n",
    "def evaluate(model, X_test, y_test):\n",
    "    score = model.score(X_test, y_test)\n",
    "    print('Accuracy = {:0.2f}%.'.format(score))\n",
    "    return score\n",
    "    \n",
    "base_model = RandomForestClassifier(n_estimators = 100)\n",
    "base_model.fit(X_train, y_train)\n",
    "base_accuracy = evaluate(base_model, X_test, y_test)\n",
    "\n",
    "random_accuracy = evaluate(rf_random.best_estimator_, X_test, y_test)\n",
    "\n",
    "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [40, 50, 60],\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [4, 5, 6],\n",
    "    'n_estimators': [1250, 1500, 2000]\n",
    "}\n",
    "\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n",
      "Elapsed time:  277.8500006198883  seconds, which is:  4.630833343664805  minutes\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 50,\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_leaf': 3,\n",
       " 'min_samples_split': 6,\n",
       " 'n_estimators': 2000}"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Elapsed time: \", end - start, \" seconds, which is: \", (end - start) / 60, \" minutes\")\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy = 0.79%.\nImprovement of -4.55%.\n"
     ]
    }
   ],
   "source": [
    "# Fit the grid search to the data\n",
    "\n",
    "grid_accuracy = evaluate(grid_search.best_estimator_, X_test, y_test)\n",
    "\n",
    "print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Cross valudate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the original Test Set\n",
    "test_to_predict = pd.read_csv(\"test.csv\", header=None)\n",
    "\n",
    "numerical_features = test_to_predict.select_dtypes(include=['float64']).columns\n",
    "categorical_features = test_to_predict.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_to_predict.loc[:, numerical_features] = iterative_numerical_imputer.transform(test_to_predict.loc[:, numerical_features])\n",
    "test_to_predict.loc[:, categorical_features] = categorical_imputer.transform(test_to_predict.loc[:, categorical_features])\n",
    "\n",
    "test_to_predict_len = len(test_to_predict)\n",
    "original = pd.concat([features, test_to_predict])\n",
    "original = pd.get_dummies(original)\n",
    "test_to_predict = X[:test_to_predict_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           X1     X2        X3     X4     X5     X6     X7        X8     X9  \\\n",
       "13  -0.557000 -1.235 -1.092000  0.344 -0.845  1.425 -0.099 -0.844000  0.537   \n",
       "63  -0.280000 -1.235 -1.092000 -1.064  0.189 -0.513 -1.196  0.237000 -0.223   \n",
       "394 -0.013959  0.627  0.348000  2.301  0.189  1.425  0.120  0.110888  0.233   \n",
       "229  1.102000 -0.366 -0.617000  2.066 -0.328 -2.451 -0.757  0.616000 -0.680   \n",
       "348 -0.557000 -1.359 -0.203325  0.266  0.835 -2.128 -0.428 -0.216000  0.689   \n",
       "..        ...    ...       ...    ...    ...    ...    ...       ...    ...   \n",
       "191  1.378000  0.006 -0.127000  0.188  0.318  0.133 -0.648  0.587000  0.081   \n",
       "367  0.179922  1.123  0.348000 -0.282  0.447 -1.159 -0.976 -0.021306 -0.071   \n",
       "260 -0.004000 -1.111 -0.127000 -0.360 -0.458  1.425 -0.648  0.689000 -0.497   \n",
       "213  1.102000 -0.118 -2.056000 -1.064  0.189 -0.513  0.120  0.324000  0.537   \n",
       "154 -0.557000  0.379  1.312000 -0.282 -0.070  0.133 -1.196 -0.844000  2.058   \n",
       "\n",
       "          X10  ...  C12_V1  C12_V2  C12_V3  C13_V1  C13_V2  C13_V3  C13_V4  \\\n",
       "13   1.234000  ...       0       0       1       0       1       0       0   \n",
       "63  -0.438000  ...       1       0       0       1       0       0       0   \n",
       "394 -0.137258  ...       1       0       0       1       0       0       0   \n",
       "229  0.784000  ...       1       0       0       0       0       0       0   \n",
       "348 -0.438000  ...       1       0       0       1       0       0       0   \n",
       "..        ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "191 -1.194000  ...       1       0       0       1       0       0       0   \n",
       "367 -0.051728  ...       0       0       1       1       0       0       0   \n",
       "260 -0.546000  ...       0       0       1       0       1       0       0   \n",
       "213  0.784000  ...       1       0       0       1       0       0       0   \n",
       "154  0.982000  ...       1       0       0       0       1       0       0   \n",
       "\n",
       "     C13_V5  C14_V1  C14_V2  \n",
       "13        0       1       0  \n",
       "63        0       1       0  \n",
       "394       0       0       1  \n",
       "229       1       0       1  \n",
       "348       0       0       1  \n",
       "..      ...     ...     ...  \n",
       "191       0       1       0  \n",
       "367       0       1       0  \n",
       "260       0       0       1  \n",
       "213       0       0       1  \n",
       "154       0       1       0  \n",
       "\n",
       "[100 rows x 67 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X1</th>\n      <th>X2</th>\n      <th>X3</th>\n      <th>X4</th>\n      <th>X5</th>\n      <th>X6</th>\n      <th>X7</th>\n      <th>X8</th>\n      <th>X9</th>\n      <th>X10</th>\n      <th>...</th>\n      <th>C12_V1</th>\n      <th>C12_V2</th>\n      <th>C12_V3</th>\n      <th>C13_V1</th>\n      <th>C13_V2</th>\n      <th>C13_V3</th>\n      <th>C13_V4</th>\n      <th>C13_V5</th>\n      <th>C14_V1</th>\n      <th>C14_V2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>13</th>\n      <td>-0.557000</td>\n      <td>-1.235</td>\n      <td>-1.092000</td>\n      <td>0.344</td>\n      <td>-0.845</td>\n      <td>1.425</td>\n      <td>-0.099</td>\n      <td>-0.844000</td>\n      <td>0.537</td>\n      <td>1.234000</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>-0.280000</td>\n      <td>-1.235</td>\n      <td>-1.092000</td>\n      <td>-1.064</td>\n      <td>0.189</td>\n      <td>-0.513</td>\n      <td>-1.196</td>\n      <td>0.237000</td>\n      <td>-0.223</td>\n      <td>-0.438000</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>394</th>\n      <td>-0.013959</td>\n      <td>0.627</td>\n      <td>0.348000</td>\n      <td>2.301</td>\n      <td>0.189</td>\n      <td>1.425</td>\n      <td>0.120</td>\n      <td>0.110888</td>\n      <td>0.233</td>\n      <td>-0.137258</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>229</th>\n      <td>1.102000</td>\n      <td>-0.366</td>\n      <td>-0.617000</td>\n      <td>2.066</td>\n      <td>-0.328</td>\n      <td>-2.451</td>\n      <td>-0.757</td>\n      <td>0.616000</td>\n      <td>-0.680</td>\n      <td>0.784000</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>348</th>\n      <td>-0.557000</td>\n      <td>-1.359</td>\n      <td>-0.203325</td>\n      <td>0.266</td>\n      <td>0.835</td>\n      <td>-2.128</td>\n      <td>-0.428</td>\n      <td>-0.216000</td>\n      <td>0.689</td>\n      <td>-0.438000</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>191</th>\n      <td>1.378000</td>\n      <td>0.006</td>\n      <td>-0.127000</td>\n      <td>0.188</td>\n      <td>0.318</td>\n      <td>0.133</td>\n      <td>-0.648</td>\n      <td>0.587000</td>\n      <td>0.081</td>\n      <td>-1.194000</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>367</th>\n      <td>0.179922</td>\n      <td>1.123</td>\n      <td>0.348000</td>\n      <td>-0.282</td>\n      <td>0.447</td>\n      <td>-1.159</td>\n      <td>-0.976</td>\n      <td>-0.021306</td>\n      <td>-0.071</td>\n      <td>-0.051728</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>260</th>\n      <td>-0.004000</td>\n      <td>-1.111</td>\n      <td>-0.127000</td>\n      <td>-0.360</td>\n      <td>-0.458</td>\n      <td>1.425</td>\n      <td>-0.648</td>\n      <td>0.689000</td>\n      <td>-0.497</td>\n      <td>-0.546000</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>213</th>\n      <td>1.102000</td>\n      <td>-0.118</td>\n      <td>-2.056000</td>\n      <td>-1.064</td>\n      <td>0.189</td>\n      <td>-0.513</td>\n      <td>0.120</td>\n      <td>0.324000</td>\n      <td>0.537</td>\n      <td>0.784000</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>154</th>\n      <td>-0.557000</td>\n      <td>0.379</td>\n      <td>1.312000</td>\n      <td>-0.282</td>\n      <td>-0.070</td>\n      <td>0.133</td>\n      <td>-1.196</td>\n      <td>-0.844000</td>\n      <td>2.058</td>\n      <td>0.982000</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 67 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "test_to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Count(1):  40 Count(-1):  60\n66.66666666666666\n"
     ]
    }
   ],
   "source": [
    "prediction = grid_search.best_estimator_.predict(test_to_predict)\n",
    "print(\"Count(1): \", np.count_nonzero(prediction == 1), \"Count(-1): \", np.count_nonzero(prediction == -1))\n",
    "print((np.count_nonzero(prediction == 1)/np.count_nonzero(prediction == -1)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting\n",
    "prediction = pd.DataFrame(prediction)\n",
    "prediction.to_csv(\"predictions.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO https://datascience.stackexchange.com/questions/75345/need-help-understanding-data-leakage\n",
    "# TODO https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74"
   ]
  }
 ]
}