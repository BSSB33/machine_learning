{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd02c829f9b014f45ac51ac6981ee268f80d78860744c6a50c09e5b990310b7d8d4",
   "display_name": "Python 3.8.8 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "2c829f9b014f45ac51ac6981ee268f80d78860744c6a50c09e5b990310b7d8d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest,chi2,RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      C1   C2   C3  C4  C5   C6   C7   C8  C9 C10  ...    X11    X12    X13  \\\n",
       "0     V1   V1   V1  V1  V1   V1   V1   V1  V1  V1  ... -0.619 -0.536 -0.092   \n",
       "1     V1   V1   V1  V1  V1   V1   V2   V1  V3  V5  ...  0.352  0.073 -0.092   \n",
       "2     V1   V1   V1  V1  V1   V1   V1   V1  V2  V2  ... -1.933 -0.536 -0.092   \n",
       "3     V1   V1   V1  V1  V1   V1   V1   V1  V2  V2  ... -0.762 -0.536 -0.092   \n",
       "4     V1   V1   V1  V1  V1   V1   V1   V1  V1  V1  ... -0.505 -0.536 -0.092   \n",
       "..   ...  ...  ...  ..  ..  ...  ...  ...  ..  ..  ...    ...    ...    ...   \n",
       "395  NaN  NaN  NaN  V3  V1  NaN  NaN  NaN  V1  V4  ...    NaN    NaN    NaN   \n",
       "396  NaN  NaN  NaN  V3  V2  NaN  NaN   V1  V3  V2  ...    NaN    NaN    NaN   \n",
       "397  NaN  NaN  NaN  V3  V2  NaN  NaN  NaN  V1  V4  ...    NaN    NaN    NaN   \n",
       "398  NaN  NaN  NaN  V3  V1  NaN  NaN  NaN  V1  V8  ...    NaN    NaN    NaN   \n",
       "399  NaN  NaN  NaN  V3  V1  NaN  NaN   V1  V3  V2  ...    NaN    NaN    NaN   \n",
       "\n",
       "       X14    X15    X16    X17    X18    X19  Y  \n",
       "0    0.182  0.034 -0.172  0.401  0.393  0.216  1  \n",
       "1    1.098  0.034  1.160  0.401  0.037  0.216  1  \n",
       "2    1.098  0.034  0.716  0.401  0.724  0.216 -1  \n",
       "3    1.098  0.034  0.716  0.401  0.712  0.216 -1  \n",
       "4    0.182  0.034  0.716  0.401  0.393  0.216  1  \n",
       "..     ...    ...    ...    ...    ...    ... ..  \n",
       "395 -2.568  0.034    NaN  0.401  1.890  0.216  1  \n",
       "396 -4.401 -2.684    NaN  0.401  1.391  0.216  1  \n",
       "397  0.548  4.926    NaN  0.401  0.992  0.216  1  \n",
       "398 -1.651  3.567    NaN  0.401  0.992  0.216  1  \n",
       "399  0.182  0.034    NaN  0.401  1.012  0.216  1  \n",
       "\n",
       "[400 rows x 34 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>C1</th>\n      <th>C2</th>\n      <th>C3</th>\n      <th>C4</th>\n      <th>C5</th>\n      <th>C6</th>\n      <th>C7</th>\n      <th>C8</th>\n      <th>C9</th>\n      <th>C10</th>\n      <th>...</th>\n      <th>X11</th>\n      <th>X12</th>\n      <th>X13</th>\n      <th>X14</th>\n      <th>X15</th>\n      <th>X16</th>\n      <th>X17</th>\n      <th>X18</th>\n      <th>X19</th>\n      <th>Y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>...</td>\n      <td>-0.619</td>\n      <td>-0.536</td>\n      <td>-0.092</td>\n      <td>0.182</td>\n      <td>0.034</td>\n      <td>-0.172</td>\n      <td>0.401</td>\n      <td>0.393</td>\n      <td>0.216</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V2</td>\n      <td>V1</td>\n      <td>V3</td>\n      <td>V5</td>\n      <td>...</td>\n      <td>0.352</td>\n      <td>0.073</td>\n      <td>-0.092</td>\n      <td>1.098</td>\n      <td>0.034</td>\n      <td>1.160</td>\n      <td>0.401</td>\n      <td>0.037</td>\n      <td>0.216</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V2</td>\n      <td>V2</td>\n      <td>...</td>\n      <td>-1.933</td>\n      <td>-0.536</td>\n      <td>-0.092</td>\n      <td>1.098</td>\n      <td>0.034</td>\n      <td>0.716</td>\n      <td>0.401</td>\n      <td>0.724</td>\n      <td>0.216</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V2</td>\n      <td>V2</td>\n      <td>...</td>\n      <td>-0.762</td>\n      <td>-0.536</td>\n      <td>-0.092</td>\n      <td>1.098</td>\n      <td>0.034</td>\n      <td>0.716</td>\n      <td>0.401</td>\n      <td>0.712</td>\n      <td>0.216</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>V1</td>\n      <td>...</td>\n      <td>-0.505</td>\n      <td>-0.536</td>\n      <td>-0.092</td>\n      <td>0.182</td>\n      <td>0.034</td>\n      <td>0.716</td>\n      <td>0.401</td>\n      <td>0.393</td>\n      <td>0.216</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>395</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>V3</td>\n      <td>V1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>V1</td>\n      <td>V4</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-2.568</td>\n      <td>0.034</td>\n      <td>NaN</td>\n      <td>0.401</td>\n      <td>1.890</td>\n      <td>0.216</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>396</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>V3</td>\n      <td>V2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>V1</td>\n      <td>V3</td>\n      <td>V2</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-4.401</td>\n      <td>-2.684</td>\n      <td>NaN</td>\n      <td>0.401</td>\n      <td>1.391</td>\n      <td>0.216</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>397</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>V3</td>\n      <td>V2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>V1</td>\n      <td>V4</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.548</td>\n      <td>4.926</td>\n      <td>NaN</td>\n      <td>0.401</td>\n      <td>0.992</td>\n      <td>0.216</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>398</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>V3</td>\n      <td>V1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>V1</td>\n      <td>V8</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-1.651</td>\n      <td>3.567</td>\n      <td>NaN</td>\n      <td>0.401</td>\n      <td>0.992</td>\n      <td>0.216</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>399</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>V3</td>\n      <td>V1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>V1</td>\n      <td>V3</td>\n      <td>V2</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.182</td>\n      <td>0.034</td>\n      <td>NaN</td>\n      <td>0.401</td>\n      <td>1.012</td>\n      <td>0.216</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>400 rows × 34 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\", header=None)\n",
    "new_cols = ['C' + str(i) for i in range(1, 14 + 1)] + ['X' + str(i) for i in range(1, 19 + 2)]\n",
    "new_cols[-1] = 'Y'\n",
    "df.columns = new_cols\n",
    "\n",
    "features = df.loc[:, df.columns != 'Y']\n",
    "numerical_features = df.select_dtypes(include=['float64']).columns\n",
    "categorical_features = df.select_dtypes(include=['object']).columns\n",
    "target = df['Y']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "68.77637130801688\n"
     ]
    }
   ],
   "source": [
    "print((np.count_nonzero(target == 1)/np.count_nonzero(target == -1)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Gábor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\impute\\_iterative.py:685: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n  warnings.warn(\"[IterativeImputer] Early stopping criterion not\"\nC:\\Users\\Gábor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._setitem_single_column(loc, value[:, i].tolist(), pi)\nC:\\Users\\Gábor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._setitem_single_column(loc, value[:, i].tolist(), pi)\nC:\\Users\\Gábor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._setitem_single_column(loc, value[:, i].tolist(), pi)\nC:\\Users\\Gábor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "#TODO Experiment with strategies\n",
    "iterative_numerical_imputer = IterativeImputer(max_iter=10)\n",
    "iterative_numerical_imputer.fit(X_train[numerical_features])\n",
    "X_train.loc[:, numerical_features] = iterative_numerical_imputer.transform(X_train.loc[:, numerical_features])\n",
    "X_test.loc[:, numerical_features] = iterative_numerical_imputer.transform(X_test.loc[:, numerical_features])\n",
    "\n",
    "categorical_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "categorical_imputer.fit(X_train[categorical_features])\n",
    "X_train.loc[:, categorical_features] = categorical_imputer.transform(X_train.loc[:, categorical_features])\n",
    "X_test.loc[:, categorical_features] = categorical_imputer.transform(X_test.loc[:, categorical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_len = len(X_train)\n",
    "X = pd.concat([X_train, X_test])\n",
    "X = pd.get_dummies(X)\n",
    "X_train = X[:X_train_len]\n",
    "X_test = X[X_train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.to_csv(\"save\\X.csv\", index=False)\n",
    "#X_train.to_csv(\"save\\X_train.csv\", index=False)\n",
    "#X_test.to_csv(\"save\\X_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Score:  0.8\n",
      "Test Score:  0.8125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classifier1 = RandomForestClassifier(n_estimators=1000)\n",
    "classifier1.fit(X_train, y_train)\n",
    "print(\"Test Score: \", classifier1.score(X_test, y_test))\n",
    "\n",
    "classifier2 = ExtraTreesClassifier(n_estimators=1000, random_state=10)\n",
    "classifier2.fit(X_train, y_train)\n",
    "print(\"Test Score: \", classifier2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RandomForestClassifier\n[[48  2]\n [14 16]]\n              precision    recall  f1-score   support\n\n          -1       0.77      0.96      0.86        50\n           1       0.89      0.53      0.67        30\n\n    accuracy                           0.80        80\n   macro avg       0.83      0.75      0.76        80\nweighted avg       0.82      0.80      0.79        80\n\nExtraTreesClassifier\n[[46  4]\n [11 19]]\n              precision    recall  f1-score   support\n\n          -1       0.81      0.92      0.86        50\n           1       0.83      0.63      0.72        30\n\n    accuracy                           0.81        80\n   macro avg       0.82      0.78      0.79        80\nweighted avg       0.81      0.81      0.81        80\n\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = classifier1.predict(X_test)\n",
    "cm1 = confusion_matrix(y_test, y_pred1)\n",
    "\n",
    "print(\"RandomForestClassifier\")\n",
    "print(cm1)\n",
    "print(classification_report(y_test, y_pred1))\n",
    "\n",
    "y_pred2 = classifier2.predict(X_test)\n",
    "cm2 = confusion_matrix(y_test, y_pred2)\n",
    "\n",
    "print(\"ExtraTreesClassifier\")\n",
    "print(cm2)\n",
    "print(classification_report(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "Elapsed time:  16.935027837753296  seconds, which is:  0.28225046396255493  minutes\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(max_depth=50, n_estimators=1100)"
      ]
     },
     "metadata": {},
     "execution_count": 123
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "parameters = {\n",
    "    'bootstrap': [False],\n",
    "    'max_depth': [50,60,70],\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [1],\n",
    "    'min_samples_split': [2],\n",
    "    'n_estimators': [1100, 1250, 1500]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = ExtraTreesClassifier(n_estimators=1000), param_grid = parameters, cv = 5, n_jobs = -1, verbose = 3)\n",
    "\n",
    "start = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "print(\"Elapsed time: \", end - start, \" seconds, which is: \", (end - start) / 60, \" minutes\")\n",
    "\n",
    "model = grid_search.best_estimator_\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Score:  0.8375\n",
      "[[47  3]\n",
      " [10 20]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.82      0.94      0.88        50\n",
      "           1       0.87      0.67      0.75        30\n",
      "\n",
      "    accuracy                           0.84        80\n",
      "   macro avg       0.85      0.80      0.82        80\n",
      "weighted avg       0.84      0.84      0.83        80\n",
      "\n",
      ">0.821 (0.041)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "clf1 = RandomForestClassifier(n_estimators=1000)\n",
    "clf2 = BaggingClassifier(n_estimators=1000)\n",
    "clf3 = ExtraTreesClassifier(n_estimators=1000)\n",
    "\n",
    "votingclf = VotingClassifier(estimators=[('rf', clf1),  ('bag', clf2), ('et', clf3), ('tuned', model)], voting='hard', weights=[2,1,2,3], flatten_transform=True)\n",
    "votingclf = votingclf.fit(X_train, y_train)\n",
    "print(\"Test Score: \", votingclf.score(X_test, y_test))\n",
    "\n",
    "y_pred_votingclf = votingclf.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred_votingclf))\n",
    "print(classification_report(y_test, y_pred_votingclf))\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "scores = cross_val_score(eclf3, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print('>%.3f (%.3f)' % (scores.mean(), scores.std()))\n",
    "\n",
    "#Test Score:  0.875\n",
    "#Test Score:  0.925\n",
    "#Test Score:  0.8375 -X\n",
    "#Test Score:  0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean Absolute Error: 0.375\nMean Squared Error: 0.75\nRoot Mean Squared Error: 0.8660254037844386\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred1))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred1))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "> RF 0.797 (0.048)\n",
      "> ET 0.796 (0.049)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def evaluate_model(X_train, y_train, X_test, y_test, model):\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\treturn cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "models, names = list(), list()\n",
    "# SVM\n",
    "#models.append(SVC(gamma='auto'))\n",
    "#names.append('SVM')\n",
    "# KNN\n",
    "#models.append(KNeighborsClassifier())\n",
    "#names.append('KNN')\n",
    "# Neutral Network\n",
    "#models.append(MLPClassifier(solver = 'adam', max_iter=1000, tol=0.000001, early_stopping=True, validation_fraction=0.1, n_iter_no_change=20))\n",
    "#names.append('NEU')\n",
    "# Bagging\n",
    "#models.append(BaggingClassifier(n_estimators=1000))\n",
    "#names.append('BAG')\n",
    "# ls.append(LogisticRegression())\n",
    "#names.append('LR')\n",
    "# RF\n",
    "models.append(RandomForestClassifier(n_estimators=1000))\n",
    "names.append(' RF')\n",
    "# ET\n",
    "models.append(ExtraTreesClassifier(n_estimators=1000, random_state=10))\n",
    "names.append(' ET')\n",
    "\n",
    "\n",
    "results = list()\n",
    "for i in range(len(models)):\n",
    "\tscores = evaluate_model(X_train, y_train, X_test, y_test, models[i])\n",
    "\tresults.append(scores)\n",
    "\tprint('>%s %.3f (%.3f)' % (names[i], scores.mean(), scores.std()))\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'n_estimators': [1000, 1250, 1500, 1750, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': [int(x) for x in np.linspace(start = 1000, stop = 2000, num = 5)],\n",
    "               'max_features': ['auto', 'sqrt'],\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': [2, 5, 10],\n",
    "               'min_samples_leaf': [1, 2, 4],\n",
    "               'bootstrap': [True, False]}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Elapsed time:  138.87605786323547  seconds, which is:  2.314600964387258  minutes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "rf = ExtraTreesClassifier()\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "start = time.time()\n",
    "\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Elapsed time: \", end - start, \" seconds, which is: \", (end - start) / 60, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'n_estimators': 1250,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 40,\n",
       " 'bootstrap': False}"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy = 0.88%.\nAccuracy = 0.88%.\nImprovement of 0.00%.\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, X_test, y_test):\n",
    "    score = model.score(X_test, y_test)\n",
    "    print('Accuracy = {:0.2f}%.'.format(score))\n",
    "    return score\n",
    "    \n",
    "base_model = ExtraTreesClassifier(n_estimators = 1000, random_state=10)\n",
    "base_model.fit(X_train, y_train)\n",
    "base_accuracy = evaluate(base_model, X_test, y_test)\n",
    "\n",
    "random_accuracy = evaluate(rf_random.best_estimator_, X_test, y_test)\n",
    "\n",
    "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [False],\n",
    "    'max_depth': [30,40,50,60,70, 80, 90, 100],\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'min_samples_split': [1, 2],\n",
    "    'n_estimators': [1250, 1500]\n",
    "}\n",
    "\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 3 folds for each of 64 candidates, totalling 192 fits\n",
      "C:\\Users\\Gábor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.82187151 0.82813143        nan        nan\n",
      " 0.81561159 0.82498677        nan        nan 0.81875625 0.81872686\n",
      "        nan        nan 0.81564098 0.81872686        nan        nan\n",
      " 0.81875625 0.82810204        nan        nan 0.82187151 0.8219009\n",
      "        nan        nan 0.82184212 0.82181273        nan        nan\n",
      " 0.82807265 0.82187151        nan        nan 0.81249633 0.81875625\n",
      "        nan        nan 0.81875625 0.81869747        nan        nan\n",
      " 0.81875625 0.82498677        nan        nan 0.82184212 0.8155822\n",
      "        nan        nan 0.82187151 0.82187151        nan        nan\n",
      " 0.82501616 0.81249633        nan        nan 0.82187151 0.82810204\n",
      "        nan        nan 0.80623641 0.81872686]\n",
      "  warnings.warn(\n",
      "Elapsed time:  50.40102028846741  seconds, which is:  0.8400170048077901  minutes\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'max_depth': 30,\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 1500}"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Elapsed time: \", end - start, \" seconds, which is: \", (end - start) / 60, \" minutes\")\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy = 0.85%.\nImprovement of -2.86%.\n"
     ]
    }
   ],
   "source": [
    "# Fit the grid search to the data\n",
    "\n",
    "grid_accuracy = evaluate(grid_search.best_estimator_, X_test, y_test)\n",
    "\n",
    "print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">Best Estimator  0.803 (0.052)\n"
     ]
    }
   ],
   "source": [
    "# Cross valudate\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "scores = cross_val_score(grid_search.best_estimator_, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print('>%s %.3f (%.3f)' % (\"Best Estimator \", scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the original Test Set\n",
    "test_to_predict = pd.read_csv(\"test.csv\", header=None)\n",
    "\n",
    "numerical_features = test_to_predict.select_dtypes(include=['float64']).columns\n",
    "categorical_features = test_to_predict.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_to_predict.loc[:, numerical_features] = iterative_numerical_imputer.transform(test_to_predict.loc[:, numerical_features])\n",
    "test_to_predict.loc[:, categorical_features] = categorical_imputer.transform(test_to_predict.loc[:, categorical_features])\n",
    "\n",
    "test_to_predict_len = len(test_to_predict)\n",
    "original = pd.concat([features, test_to_predict])\n",
    "original = pd.get_dummies(original)\n",
    "test_to_predict = X[:test_to_predict_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           X1     X2        X3        X4     X5     X6      X7        X8  \\\n",
       "126  1.931000 -0.490 -1.567000  1.283000 -0.458  0.133 -0.0990  1.245000   \n",
       "395 -0.322540  0.130 -0.211433  2.066000  0.447  1.425 -0.6480 -0.024088   \n",
       "84  -0.336000 -0.862  0.348000 -0.282000  0.189  2.717 -1.1960  1.595000   \n",
       "227  0.272000  0.254  0.348000 -0.282000 -0.845  0.133 -0.6480  2.369000   \n",
       "115  1.378000  0.503  0.823000 -1.064000  0.965  0.133 -0.0990  1.055000   \n",
       "..        ...    ...       ...       ...    ...    ...     ...       ...   \n",
       "251 -0.557000 -0.986 -0.127000  1.440000 -0.070  1.102 -0.0969 -0.698000   \n",
       "344  0.061034 -0.862 -1.092000 -0.829000 -0.070 -2.451  0.1200 -0.844000   \n",
       "205  2.207000 -0.118 -0.127000 -0.282000  0.965 -2.451 -0.3190  0.689000   \n",
       "185 -0.004000  0.627  0.823000 -0.360000  0.835  1.425 -0.0990  1.683000   \n",
       "234  1.655000 -0.614  0.823000 -0.105739 -1.104 -0.513  2.0930  1.931000   \n",
       "\n",
       "        X9       X10  ...  C12_V1  C12_V2  C12_V3  C13_V1  C13_V2  C13_V3  \\\n",
       "126 -0.375 -1.445000  ...       1       0       0       1       0       0   \n",
       "395  0.172 -0.066155  ...       1       0       0       1       0       0   \n",
       "84  -1.592 -1.194000  ...       1       0       0       1       0       0   \n",
       "227 -0.984 -1.751000  ...       1       0       0       0       0       0   \n",
       "115 -0.375 -1.014000  ...       0       1       0       1       0       0   \n",
       "..     ...       ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "251  1.085  0.479000  ...       1       0       0       0       1       0   \n",
       "344 -0.527  0.209000  ...       0       1       0       1       0       0   \n",
       "205 -0.375 -0.546000  ...       0       1       0       1       0       0   \n",
       "185 -0.680 -1.356000  ...       1       0       0       1       0       0   \n",
       "234 -1.315 -1.445000  ...       0       1       0       0       0       0   \n",
       "\n",
       "     C13_V4  C13_V5  C14_V1  C14_V2  \n",
       "126       0       0       1       0  \n",
       "395       0       0       0       1  \n",
       "84        0       0       0       1  \n",
       "227       0       1       1       0  \n",
       "115       0       0       0       1  \n",
       "..      ...     ...     ...     ...  \n",
       "251       0       0       0       1  \n",
       "344       0       0       0       1  \n",
       "205       0       0       0       1  \n",
       "185       0       0       0       1  \n",
       "234       0       1       1       0  \n",
       "\n",
       "[100 rows x 67 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X1</th>\n      <th>X2</th>\n      <th>X3</th>\n      <th>X4</th>\n      <th>X5</th>\n      <th>X6</th>\n      <th>X7</th>\n      <th>X8</th>\n      <th>X9</th>\n      <th>X10</th>\n      <th>...</th>\n      <th>C12_V1</th>\n      <th>C12_V2</th>\n      <th>C12_V3</th>\n      <th>C13_V1</th>\n      <th>C13_V2</th>\n      <th>C13_V3</th>\n      <th>C13_V4</th>\n      <th>C13_V5</th>\n      <th>C14_V1</th>\n      <th>C14_V2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>126</th>\n      <td>1.931000</td>\n      <td>-0.490</td>\n      <td>-1.567000</td>\n      <td>1.283000</td>\n      <td>-0.458</td>\n      <td>0.133</td>\n      <td>-0.0990</td>\n      <td>1.245000</td>\n      <td>-0.375</td>\n      <td>-1.445000</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>395</th>\n      <td>-0.322540</td>\n      <td>0.130</td>\n      <td>-0.211433</td>\n      <td>2.066000</td>\n      <td>0.447</td>\n      <td>1.425</td>\n      <td>-0.6480</td>\n      <td>-0.024088</td>\n      <td>0.172</td>\n      <td>-0.066155</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>-0.336000</td>\n      <td>-0.862</td>\n      <td>0.348000</td>\n      <td>-0.282000</td>\n      <td>0.189</td>\n      <td>2.717</td>\n      <td>-1.1960</td>\n      <td>1.595000</td>\n      <td>-1.592</td>\n      <td>-1.194000</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>227</th>\n      <td>0.272000</td>\n      <td>0.254</td>\n      <td>0.348000</td>\n      <td>-0.282000</td>\n      <td>-0.845</td>\n      <td>0.133</td>\n      <td>-0.6480</td>\n      <td>2.369000</td>\n      <td>-0.984</td>\n      <td>-1.751000</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>1.378000</td>\n      <td>0.503</td>\n      <td>0.823000</td>\n      <td>-1.064000</td>\n      <td>0.965</td>\n      <td>0.133</td>\n      <td>-0.0990</td>\n      <td>1.055000</td>\n      <td>-0.375</td>\n      <td>-1.014000</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>251</th>\n      <td>-0.557000</td>\n      <td>-0.986</td>\n      <td>-0.127000</td>\n      <td>1.440000</td>\n      <td>-0.070</td>\n      <td>1.102</td>\n      <td>-0.0969</td>\n      <td>-0.698000</td>\n      <td>1.085</td>\n      <td>0.479000</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>344</th>\n      <td>0.061034</td>\n      <td>-0.862</td>\n      <td>-1.092000</td>\n      <td>-0.829000</td>\n      <td>-0.070</td>\n      <td>-2.451</td>\n      <td>0.1200</td>\n      <td>-0.844000</td>\n      <td>-0.527</td>\n      <td>0.209000</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>205</th>\n      <td>2.207000</td>\n      <td>-0.118</td>\n      <td>-0.127000</td>\n      <td>-0.282000</td>\n      <td>0.965</td>\n      <td>-2.451</td>\n      <td>-0.3190</td>\n      <td>0.689000</td>\n      <td>-0.375</td>\n      <td>-0.546000</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>185</th>\n      <td>-0.004000</td>\n      <td>0.627</td>\n      <td>0.823000</td>\n      <td>-0.360000</td>\n      <td>0.835</td>\n      <td>1.425</td>\n      <td>-0.0990</td>\n      <td>1.683000</td>\n      <td>-0.680</td>\n      <td>-1.356000</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>234</th>\n      <td>1.655000</td>\n      <td>-0.614</td>\n      <td>0.823000</td>\n      <td>-0.105739</td>\n      <td>-1.104</td>\n      <td>-0.513</td>\n      <td>2.0930</td>\n      <td>1.931000</td>\n      <td>-1.315</td>\n      <td>-1.445000</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 67 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "test_to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Count(1):  42 Count(-1):  58\n72.41379310344827\n"
     ]
    }
   ],
   "source": [
    "prediction = grid_search.best_estimator_.predict(test_to_predict)\n",
    "print(\"Count(1): \", np.count_nonzero(prediction == 1), \"Count(-1): \", np.count_nonzero(prediction == -1))\n",
    "print((np.count_nonzero(prediction == 1)/np.count_nonzero(prediction == -1)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting\n",
    "prediction = pd.DataFrame(prediction)\n",
    "prediction.to_csv(\"predictions.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO https://datascience.stackexchange.com/questions/75345/need-help-understanding-data-leakage\n",
    "# TODO https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74"
   ]
  }
 ]
}