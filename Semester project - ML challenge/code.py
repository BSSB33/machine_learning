# -*- coding: utf-8 -*-
"""
code.ipynb
Automatically generated by Colaboratory FORM JUPYTER NOTEBOOK.
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from collections import Counter
from numpy import mean
from numpy import std
from pandas import read_csv
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import SelectKBest,chi2,RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder

df = pd.read_csv("train.csv", header=None)
new_cols = ['C' + str(i) for i in range(1, 14 + 1)] + ['X' + str(i) for i in range(1, 19 + 2)]
new_cols[-1] = 'Y'
df.columns = new_cols

features = df.loc[:, df.columns != 'Y']

features = features.drop(['C2', 'C3', 'C6', 'C7', 'X19'], axis=1)

numerical_features = features.select_dtypes(include=['float64']).columns
categorical_features = features.select_dtypes(include=['object']).columns
target = df['Y']

features

print((np.count_nonzero(target == 1)/np.count_nonzero(target == -1)) * 100)

X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=32, test_size=0.2) #32 is good

from sklearn.impute import SimpleImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

#TODO Experiment with strategies
iterative_numerical_imputer = IterativeImputer(max_iter=100, initial_strategy='median')
iterative_numerical_imputer.fit(X_train[numerical_features])
X_train.loc[:, numerical_features] = iterative_numerical_imputer.transform(X_train.loc[:, numerical_features])
X_test.loc[:, numerical_features] = iterative_numerical_imputer.transform(X_test.loc[:, numerical_features])

categorical_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
categorical_imputer.fit(X_train[categorical_features])
X_train.loc[:, categorical_features] = categorical_imputer.transform(X_train.loc[:, categorical_features])
X_test.loc[:, categorical_features] = categorical_imputer.transform(X_test.loc[:, categorical_features])

X_train

X_train_len = len(X_train)
X = pd.concat([X_train, X_test])
X = pd.get_dummies(X)
X_train = X[:X_train_len]
X_test = X[X_train_len:]

#X.to_csv("save\X.csv", index=False)
#X_train.to_csv("save\X_train.csv", index=False)
#X_test.to_csv("save\X_test.csv", index=False)

clf = RandomForestClassifier()
clf.fit(X_train, y_train)
plt.figure(figsize=(10,5))
plt.bar(X_train.columns, clf.feature_importances_)
plt.xticks(rotation=90)

from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.neural_network import MLPClassifier

classifier1 = RandomForestClassifier(n_estimators=1000)
classifier1.fit(X_train, y_train)
print("Test Score: ", classifier1.score(X_test, y_test))

classifier2 = ExtraTreesClassifier(n_estimators=1000)
classifier2.fit(X_train, y_train)
print("Test Score: ", classifier2.score(X_test, y_test))

y_pred1 = classifier1.predict(X_test)
cm1 = confusion_matrix(y_test, y_pred1)

print("RandomForestClassifier")
print(cm1)
print(classification_report(y_test, y_pred1))

y_pred2 = classifier2.predict(X_test)
cm2 = confusion_matrix(y_test, y_pred2)

print("ExtraTreesClassifier")
print(cm2)
print(classification_report(y_test, y_pred2))

from sklearn.model_selection import GridSearchCV
import time

parameters = {
    'bootstrap': [False],
    'max_depth': [50,60,70],
    'max_features': ['auto'],
    'min_samples_leaf': [1],
    'min_samples_split': [2],
    'n_estimators': [1100, 1250, 1500]
}

grid_search = GridSearchCV(estimator = ExtraTreesClassifier(n_estimators=1000), param_grid = parameters, cv = 5, n_jobs = -1, verbose = 3)

start = time.time()
grid_search.fit(X_train, y_train)
end = time.time()
print("Elapsed time: ", end - start, " seconds, which is: ", (end - start) / 60, " minutes")

model = grid_search.best_estimator_
model

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier, VotingClassifier

clf1 = RandomForestClassifier(max_depth=60, n_estimators=1100)
clf2 = BaggingClassifier(n_estimators=1000)
clf3 = ExtraTreesClassifier(max_depth=60, n_estimators=1100)

votingclf = VotingClassifier(estimators=[('rf', clf1),  ('bag', clf2), ('et', clf3), ('tuned', model)], voting='hard', weights=[2,1,2,3], flatten_transform=True)
votingclf = votingclf.fit(X_train, y_train)
print("Test Score: ", votingclf.score(X_test, y_test))

y_pred_votingclf = votingclf.predict(X_test)
print(confusion_matrix(y_test, y_pred_votingclf))
print(classification_report(y_test, y_pred_votingclf))
"""
cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)
scores = cross_val_score(votingclf, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)
print('>%.3f (%.3f)' % (scores.mean(), scores.std()))
"""
#Test Score:  0.875
#Test Score:  0.8875
# TODO Check slides, train on full dataset

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import plot_roc_curve
from sklearn.datasets import load_wine
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier

ax = plt.gca()
rfc_disp = plot_roc_curve(classifier1, X_test, y_test, ax=ax, alpha=0.8)
rfc_disp = plot_roc_curve(classifier2, X_test, y_test, ax=ax, alpha=0.8)
#svc_disp.plot(ax=ax, alpha=0.8)

from sklearn import metrics

print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred1))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred1))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred1)))

from sklearn.linear_model import LogisticRegression

def evaluate_model(X_train, y_train, X_test, y_test, model):
	cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)
	return cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)

models, names = list(), list()
# SVM
#models.append(SVC(gamma='auto'))
#names.append('SVM')
# KNN
#models.append(KNeighborsClassifier())
#names.append('KNN')
# Neutral Network
#models.append(MLPClassifier(solver = 'adam', max_iter=1000, tol=0.000001, early_stopping=True, validation_fraction=0.1, n_iter_no_change=20))
#names.append('NEU')
# Bagging
#models.append(BaggingClassifier(n_estimators=1000))
#names.append('BAG')
# LogReg
#models.append(LogisticRegression())
#names.append('LR')
# RF
#models.append(RandomForestClassifier(n_estimators=1000))
#names.append(' RF')
# ET
models.append(ExtraTreesClassifier(n_estimators=1000))
names.append(' ET')


results = list()
for i in range(len(models)):
	scores = evaluate_model(X_train, y_train, X_test, y_test, models[i])
	results.append(scores)
	print('>%s %.3f (%.3f)' % (names[i], scores.mean(), scores.std()))

from sklearn.model_selection import RandomizedSearchCV

# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)

# Create the random grid
random_grid = {'n_estimators': [int(x) for x in np.linspace(start = 1000, stop = 2000, num = 5)],
               'max_features': ['auto', 'sqrt'],
               'max_depth': max_depth,
               'min_samples_split': [2, 5, 10],
               'min_samples_leaf': [1, 2, 4],
               'bootstrap': [True, False]}

print(random_grid)

import time

rf = ExtraTreesClassifier()

# Random search of parameters, using 3 fold cross validation, search across 100 different combinations, and use all available cores
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)

# Fit the random search model
start = time.time()

rf_random.fit(X_train, y_train)

end = time.time()
print("Elapsed time: ", end - start, " seconds, which is: ", (end - start) / 60, " minutes")

rf_random.best_params_

def evaluate(model, X_test, y_test):
    score = model.score(X_test, y_test)
    print('Accuracy = {:0.2f}%.'.format(score))
    return score
    
base_model = ExtraTreesClassifier(n_estimators = 1000, random_state=10)
base_model.fit(X_train, y_train)
base_accuracy = evaluate(base_model, X_test, y_test)

random_accuracy = evaluate(rf_random.best_estimator_, X_test, y_test)

print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))

from sklearn.model_selection import GridSearchCV

# Create the parameter grid based on the results of random search 
param_grid = {
    'bootstrap': [False],
    'max_depth': [30,40,50,60,70, 80, 90, 100],
    'max_features': ['auto'],
    'min_samples_leaf': [1, 2],
    'min_samples_split': [1, 2],
    'n_estimators': [1250, 1500]
}

# Create a based model
rf = RandomForestClassifier()
# Instantiate the grid search model
grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 3)

start = time.time()

grid_search.fit(X_train, y_train)

end = time.time()
print("Elapsed time: ", end - start, " seconds, which is: ", (end - start) / 60, " minutes")

grid_search.best_params_

# Fit the grid search to the data

grid_accuracy = evaluate(grid_search.best_estimator_, X_test, y_test)

print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))
print(confusion_matrix(y_test, y_pred_votingclf))

# Cross valudate
cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)
scores = cross_val_score(grid_search.best_estimator_, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)
print('>%s %.3f (%.3f)' % ("Best Estimator ", scores.mean(), scores.std()))

"""# Predicting the original Test Set with X_train"""

test_to_predict = pd.read_csv("test.csv", header=None)
new_cols = ['C' + str(i) for i in range(1, 14 + 1)] + ['X' + str(i) for i in range(1, 19 + 1)]
test_to_predict.columns = new_cols

test_to_predict = test_to_predict.drop(['C2', 'C3', 'C6', 'C7', 'X19'], axis=1)

numerical_features = test_to_predict.select_dtypes(include=['float64']).columns
categorical_features = test_to_predict.select_dtypes(include=['object']).columns
test_to_predict

test_to_predict.loc[:, numerical_features] = iterative_numerical_imputer.transform(test_to_predict.loc[:, numerical_features])
test_to_predict.loc[:, categorical_features] = categorical_imputer.transform(test_to_predict.loc[:, categorical_features])

test_to_predict_len = len(test_to_predict)
original = pd.concat([features, test_to_predict])
original = pd.get_dummies(original)
test_to_predict = X[:test_to_predict_len]

test_to_predict

prediction = grid_search.best_estimator_.predict(test_to_predict)
print("Count(1): ", np.count_nonzero(prediction == 1), "Count(-1): ", np.count_nonzero(prediction == -1))
print((np.count_nonzero(prediction == 1)/np.count_nonzero(prediction == -1)) * 100)

# Exporting
prediction = pd.DataFrame(prediction)
prediction.to_csv("predictions.csv", index=False, header=False)

# TODO https://datascience.stackexchange.com/questions/75345/need-help-understanding-data-leakage
# TODO https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74

"""# Predicting the Test Set with Full Training set"""

df = pd.read_csv("train.csv", header=None)
new_cols = ['C' + str(i) for i in range(1, 14 + 1)] + ['X' + str(i) for i in range(1, 19 + 2)]
new_cols[-1] = 'Y'
df.columns = new_cols

train = df.loc[:, df.columns != 'Y']

train = train.drop(['C2', 'C3', 'C6', 'C7', 'X19'], axis=1)

numerical_features = train.select_dtypes(include=['float64']).columns
categorical_features = train.select_dtypes(include=['object']).columns
y = df['Y']

train

test = pd.read_csv("test.csv", header=None)
new_cols = ['C' + str(i) for i in range(1, 14 + 1)] + ['X' + str(i) for i in range(1, 19 + 1)]
test.columns = new_cols

test = test.drop(['C2', 'C3', 'C6', 'C7', 'X19'], axis=1)

numerical_features = test.select_dtypes(include=['float64']).columns
categorical_features = test.select_dtypes(include=['object']).columns

test

from sklearn.impute import SimpleImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

iterative_numerical_imputer = IterativeImputer(max_iter=1000, initial_strategy='median')
iterative_numerical_imputer.fit(features[numerical_features])
train.loc[:, numerical_features] = iterative_numerical_imputer.transform(train.loc[:, numerical_features])
test.loc[:, numerical_features] = iterative_numerical_imputer.transform(test.loc[:, numerical_features])

categorical_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
categorical_imputer.fit(features[categorical_features])
train.loc[:, categorical_features] = categorical_imputer.transform(train.loc[:, categorical_features])
test.loc[:, categorical_features] = categorical_imputer.transform(test.loc[:, categorical_features])

train

train_len = len(train)
X = pd.concat([train, test])
X = pd.get_dummies(X)
train = X[:train_len]
test = X[train_len:]

from sklearn.model_selection import GridSearchCV
import time

parameters = {
    'bootstrap': [False],
    'max_depth': [50, 60, 70],
    'max_features': ['auto'],
    'min_samples_leaf': [1, 2],
    'min_samples_split': [2, 3],
    'n_estimators': [1100, 1250, 1500]
}

grid_search = GridSearchCV(estimator = ExtraTreesClassifier(n_estimators=1000), param_grid = parameters, cv = 5, n_jobs = -1, verbose = 3)

start = time.time()
grid_search.fit(train, y)
end = time.time()
print("Elapsed time: ", end - start, " seconds, which is: ", (end - start) / 60, " minutes")

model = grid_search.best_estimator_
model

from sklearn.ensemble import RandomForestClassifier, VotingClassifier

clf1 = RandomForestClassifier(max_depth=60, n_estimators=1100)
clf2 = BaggingClassifier(n_estimators=1000)
clf3 = ExtraTreesClassifier(n_estimators=1000)

votingclf = VotingClassifier(estimators=[('rf', clf1),  ('bag', clf2), ('et', clf3), ('tuned', model)], voting='hard', weights=[2,1,2,3], flatten_transform=True)
votingclf = votingclf.fit(train, y)

y_pred = votingclf.predict(test)

cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)
scores = cross_val_score(votingclf, train, y, scoring='accuracy', cv=cv, n_jobs=-1)
print('>%.3f (%.3f)' % (scores.mean(), scores.std()))

# TODO Check slides
#>0.831 (0.032)
#>0.818 (0.043)

cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)
scores = cross_val_score(BaggingClassifier(n_estimators=1000), train, y, scoring='accuracy', cv=cv, n_jobs=-1)
print('>%.3f (%.3f)' % (scores.mean(), scores.std()))

# Exporting
y_pred = pd.DataFrame(y_pred)
y_pred.to_csv("y_pred.csv", index=False, header=False)