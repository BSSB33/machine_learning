{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab3a.ipynb másolata",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK4TY8M-xt9d"
      },
      "source": [
        "# Deep learning (visszacsatolt hálók)\n",
        "\n",
        "A gyakorlaton egy egyszerű, deep learning-alapú nyelvi modellt valósítunk meg, amellyel magyar nyelvű szövegeket dolgozunk fel. Az implementáció alapját egy visszacsatolt neurális háló adja, amelyet a szövegben soron következő karakter predikciójára tanítunk (a korábban megfigyelt karaktereket, mint bemenetet felhasználva). A tanult modellt ezután szöveg generálására használjuk fel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPkT1IQsxvj0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd3e0c7e-403c-44e5-992f-8c5d423b4524"
      },
      "source": [
        "%pylab inline\n",
        "\n",
        "import tensorflow as tf\n",
        "from urllib.request import urlopen"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZiDtYSvz4PN"
      },
      "source": [
        "## 1. Bemeneti adatok előállítása\n",
        "\n",
        "A karaktereket one-hot módon kódoljuk. Minden karaktert egy $\\left\\lbrace 0,1\\right\\rbrace^d$ vektorral reprezentálunk, ahol például\n",
        "\n",
        "\\begin{align}\n",
        "\\text{'a'} &= \\left[1, 0, 0, \\dots, 0 \\right], \\\\\n",
        "\\text{'b'} &= \\left[0, 1, 0, \\dots, 0 \\right], \\\\\n",
        "& \\vdots \\\\\n",
        "\\text{'Z'} &= \\left[0, 0, 0, \\dots, 1 \\right], \\\\\n",
        "\\end{align}\n",
        "\n",
        "A visszacsatolt hálózat bemenetére egy $\\left(n,m,d\\right)$ méretű, kimenetére egy $\\left(n,d\\right)$ méretű tenzor kerül, ahol \n",
        "\n",
        "- $n$ a tanítóminták száma\n",
        "- Minden tanítómintában van a teljes szövegnek egy $m$ hosszú rész-szekvenciája, amelynek minden karakterét az előbb látott $d$-dimenziós vektorokkal kódoljuk (bemenet)\n",
        "- Minden tanítómintában található egy $d$-dimenziós vektor (kimenet), amely az előbbi rész-szekvencia utáni első rákövetkező karaktert reprezentálja.\n",
        "\n",
        "A tanítómintákat úgy generáljuk, hogy ezt az $m$ széles ablakot végigtoljuk a teljes szövegen. Összefoglalva, a háló a bemenet-kimenet összefüggés becslése során $m$ lépésnyit tekint vissza."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TriJAEKy2SYR"
      },
      "source": [
        "url = \"https://www.mit.bme.hu/system/files/oktatas/targyak/10142/telep.txt\" # nevek.txt, telep.txt\n",
        "seq = urlopen(url).read().decode('utf8')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu8akt6vz5z4"
      },
      "source": [
        "<b>1.1. feladat.</b> Hozzon létre egy kódoló és dekódoló dictionary-t a one-hot kódoláshoz (azaz előbbiben minden egyedi karakterhez szerepeljen egy egyedi $d$-nél kisebb szám, utóbbi legyen az előbbi inverze)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5azI28Zzz4u"
      },
      "source": [
        "chars = set(seq)\n",
        "d     = len(chars)\n",
        "\n",
        "encoding = dict(zip(chars,range(d)))\n",
        "decoding = dict(zip(range(d),chars))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiQ1vsww0G0m"
      },
      "source": [
        "<b>1.2. feladat.</b> Alkalmas $m$ választása mellett hozza létre háló bemenetére kerülő $(n,m,d)$ méretű tenzort, valamint a kimenetet reprezentáló $(n,d)$ méretű tenzort!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M5_2mR00DgX"
      },
      "source": [
        "l = len(seq)\n",
        "m = 32\n",
        "w = 2\n",
        "n = (l-m)//w\n",
        "\n",
        "chars_in  = np.zeros((n,m,d),dtype=np.bool)\n",
        "chars_out = np.zeros((n,d),dtype=np.bool)\n",
        "\n",
        "for i in range(n):\n",
        "    for j in range(m):\n",
        "        chars_in[i,j, encoding[seq[i * w + j]]] = 1\n",
        "    chars_out[i, encoding[seq[i * w + m]]] = 1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8PkL43_0M6M"
      },
      "source": [
        "## 2. Visszacsatolt háló létrehozása\n",
        "\n",
        "A hálózathoz tetszőleges architektúrát használhatunk, de ügyelnünk kell arra, hogy a hálózat kimenete kompatibilis legyen az előző lépésben létrehozott kimenettel, azaz a kimenet mérete legyen $d$ (ehhez például illeszthetünk egy teljesen összekötött réteget a visszacsatolt réteg után). Mivel lényegében többosztályos osztályozást végzünk, a veszteségfüggvényt és aktivációs függvényt is ennek megfelelően kell megválasztani. A hálózat létrehozásához itt talál segítséget:\n",
        "\n",
        "https://keras.io/\n",
        "\n",
        "(pl. Input, LSTM/GRU/SimpleRNN, Dense rétegek)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTIYk5R10Qh4"
      },
      "source": [
        "<b>2.1. feladat.</b> Hozzon létre a követelményeknek megfelelő neurális hálózatot, majd hozzon létre egy modellt (`tf.keras.Sequential()`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DmJEtooyM0G"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(m,d)),\n",
        "    tf.keras.layers.LSTM(64),\n",
        "    tf.keras.layers.Dense(d)\n",
        "])\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam',loss=loss)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zgn4oFia0V11"
      },
      "source": [
        "<b>2.2. feladat.</b> Végezze el a tanítást (<i>tf.keras.Model.fit()</i> függvény)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrXhqb4Zm1A8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c474096a-645a-403d-d244-76e0813bf715"
      },
      "source": [
        "model.fit(chars_in,chars_out,epochs=50,batch_size=256)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "59/59 [==============================] - 5s 77ms/step - loss: 2.1263\n",
            "Epoch 2/50\n",
            "59/59 [==============================] - 5s 78ms/step - loss: 2.1214\n",
            "Epoch 3/50\n",
            "59/59 [==============================] - 4s 74ms/step - loss: 2.1180\n",
            "Epoch 4/50\n",
            "59/59 [==============================] - 4s 74ms/step - loss: 2.1132\n",
            "Epoch 5/50\n",
            "59/59 [==============================] - 5s 79ms/step - loss: 2.1096\n",
            "Epoch 6/50\n",
            "59/59 [==============================] - 5s 79ms/step - loss: 2.1066\n",
            "Epoch 7/50\n",
            "59/59 [==============================] - 5s 80ms/step - loss: 2.0992\n",
            "Epoch 8/50\n",
            "59/59 [==============================] - 5s 78ms/step - loss: 2.0945\n",
            "Epoch 9/50\n",
            "59/59 [==============================] - 5s 76ms/step - loss: 2.0923\n",
            "Epoch 10/50\n",
            "59/59 [==============================] - 4s 75ms/step - loss: 2.0872\n",
            "Epoch 11/50\n",
            "59/59 [==============================] - 4s 75ms/step - loss: 2.0844\n",
            "Epoch 12/50\n",
            "59/59 [==============================] - 4s 74ms/step - loss: 2.0781\n",
            "Epoch 13/50\n",
            "59/59 [==============================] - 5s 77ms/step - loss: 2.0767\n",
            "Epoch 14/50\n",
            "59/59 [==============================] - 5s 79ms/step - loss: 2.0722\n",
            "Epoch 15/50\n",
            "59/59 [==============================] - 5s 82ms/step - loss: 2.0668\n",
            "Epoch 16/50\n",
            "59/59 [==============================] - 5s 82ms/step - loss: 2.0645\n",
            "Epoch 17/50\n",
            "59/59 [==============================] - 4s 75ms/step - loss: 2.0587\n",
            "Epoch 18/50\n",
            "59/59 [==============================] - 5s 78ms/step - loss: 2.0550\n",
            "Epoch 19/50\n",
            "59/59 [==============================] - 4s 74ms/step - loss: 2.0557\n",
            "Epoch 20/50\n",
            "59/59 [==============================] - 4s 73ms/step - loss: 2.0488\n",
            "Epoch 21/50\n",
            "59/59 [==============================] - 5s 77ms/step - loss: 2.0432\n",
            "Epoch 22/50\n",
            "59/59 [==============================] - 4s 76ms/step - loss: 2.0415\n",
            "Epoch 23/50\n",
            "59/59 [==============================] - 4s 76ms/step - loss: 2.0385\n",
            "Epoch 24/50\n",
            "59/59 [==============================] - 4s 76ms/step - loss: 2.0323\n",
            "Epoch 25/50\n",
            "59/59 [==============================] - 4s 74ms/step - loss: 2.0312\n",
            "Epoch 26/50\n",
            "59/59 [==============================] - 5s 76ms/step - loss: 2.0273\n",
            "Epoch 27/50\n",
            "59/59 [==============================] - 4s 76ms/step - loss: 2.0246\n",
            "Epoch 28/50\n",
            "59/59 [==============================] - 4s 74ms/step - loss: 2.0189\n",
            "Epoch 29/50\n",
            "59/59 [==============================] - 4s 75ms/step - loss: 2.0145\n",
            "Epoch 30/50\n",
            "59/59 [==============================] - 4s 74ms/step - loss: 2.0124\n",
            "Epoch 31/50\n",
            "59/59 [==============================] - 4s 75ms/step - loss: 2.0088\n",
            "Epoch 32/50\n",
            "59/59 [==============================] - 4s 75ms/step - loss: 2.0045\n",
            "Epoch 33/50\n",
            "59/59 [==============================] - 4s 75ms/step - loss: 2.0008\n",
            "Epoch 34/50\n",
            "59/59 [==============================] - 4s 76ms/step - loss: 1.9969\n",
            "Epoch 35/50\n",
            "59/59 [==============================] - 4s 75ms/step - loss: 1.9948\n",
            "Epoch 36/50\n",
            "59/59 [==============================] - 4s 75ms/step - loss: 1.9883\n",
            "Epoch 37/50\n",
            "59/59 [==============================] - 4s 74ms/step - loss: 1.9859\n",
            "Epoch 38/50\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 1.9834\n",
            "Epoch 39/50\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 1.9793\n",
            "Epoch 40/50\n",
            "59/59 [==============================] - 4s 75ms/step - loss: 1.9755\n",
            "Epoch 41/50\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 1.9717\n",
            "Epoch 42/50\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 1.9679\n",
            "Epoch 43/50\n",
            "59/59 [==============================] - 4s 74ms/step - loss: 1.9624\n",
            "Epoch 44/50\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 1.9587\n",
            "Epoch 45/50\n",
            "59/59 [==============================] - 4s 74ms/step - loss: 1.9549\n",
            "Epoch 46/50\n",
            "59/59 [==============================] - 4s 75ms/step - loss: 1.9502\n",
            "Epoch 47/50\n",
            "59/59 [==============================] - 5s 76ms/step - loss: 1.9465\n",
            "Epoch 48/50\n",
            "59/59 [==============================] - 4s 76ms/step - loss: 1.9404\n",
            "Epoch 49/50\n",
            "59/59 [==============================] - 4s 73ms/step - loss: 1.9381\n",
            "Epoch 50/50\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 1.9302\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7d0cc637f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MvoahOk0odP"
      },
      "source": [
        "## 3. Szöveg generálása\n",
        "\n",
        "A szöveg generálásához a következőképpen járhatunk el:\n",
        "\n",
        "- Az első bemenetet létrehozhatjuk tetszőlegesen (pl. a szöveg egy részlete)\n",
        "- Ezen bemenetet a tanult modellre ráadva megkapjuk a rákövetkező karakter eloszlását\n",
        "- Az eloszlásból mintavételezzük a rákövetkező karaktert, amit ki is írunk\n",
        "- A bemenetet shifteljük (az utolsó helyre az imént kapott karakter kerül, az első pedig kiesik)\n",
        "\n",
        "Ezt tetszőleges ideig ismételgetjük."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_wRswj20p-e"
      },
      "source": [
        "<b>3.1. feladat.</b> Generáljon 200 karakternyi szöveget a tanult modell felhasználásával. Variálja az architektúrát, dokumentálja a tapasztaltakat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ain9MBqWA6Hs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7d5f5f6-bc69-4434-9e83-98282c9cee7f"
      },
      "source": [
        "num_text = 200\n",
        "final_text = \"\"\n",
        "\n",
        "x_pred = np.zeros((1, m, d))\n",
        "# Véletlenszerű első bemenet\n",
        "for i in range(32):\n",
        "  first_inputs = np.zeros(d)\n",
        "  first_inputs[int(round(rand()*(d-1)))] = 1\n",
        "  x_pred[0, i, :] = first_inputs\n",
        "\n",
        "# 1) Distribution 2) sampling the next character  3) shifting\n",
        "for i in range(num_text):\n",
        "  pred = model.predict(x_pred)[0]\n",
        "  pred_index = np.argmax(pred)\n",
        "  final_text += decoding[pred_index]\n",
        "  newChar = np.zeros((d))\n",
        "  newChar[pred_index] = 1\n",
        "  \n",
        "  x_pred = np.roll(x_pred, -1, axis=1) #shift\n",
        "  x_pred[0, m-1, :] = newChar\n",
        "\n",
        "# Output\n",
        "print(final_text)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "las\n",
            "Szarasztúr\n",
            "Kisboros\n",
            "Szentereszt\n",
            "Szalatenteresz\n",
            "Kiskorád\n",
            "Szentereszt\n",
            "Szaka\n",
            "Kisbordony\n",
            "Szententere\n",
            "Kiskeresztente\n",
            "Belesk\n",
            "Balatanyente\n",
            "Tárád\n",
            "Szentereny\n",
            "Szentereszt\n",
            "Szalat\n",
            "Szány\n",
            "Szentgye\n",
            "Belyő\n",
            "Balatat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_N3dE_GBnsuj"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqqcpmI6kRam"
      },
      "source": [
        "Először a kapott (1db 64-es LSTM réteg) paraméterekkel próbáltam, magas keresztentrópiát észleltem. A fenti kiemenet két (64-es) réteg melletti eredményt mutat, mely lényegesen jobban teljesít. Egy példát csináltam egy 32-es LSTM-re:\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        ">ala\r\n",
        "Kiszalaszenteny\r\n",
        "Szalaszenteny\r\n",
        "Szerenteny\r\n",
        "Szerenteny\r\n",
        "Szerenteny\r\n",
        "Szerenteny\r\n",
        "Szerenteny\r\n",
        "Szerenteny\r\n",
        "Szerenteny\r\n",
        "Szerenteny\r\n",
        "Szerenteny\r\n",
        "Szerenteny\r\n",
        "Szerenteny\r\n",
        "Szerenteny\r\n",
        "Szerenteny\r\n",
        "Szerenteny\r\n",
        "Szerenteny\r\n",
        "S\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "^ mint látszik, a felhasználhatóság mércéjét nem igazán üti meg."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2x1-lQEJ0vJg"
      },
      "source": [
        "<b>3.2. szorgalmi.</b> Hozzon létre és tanítson egy bonyolultabb nyelvi modellt, amely hosszabb szövegek, pl. könyvek generálására is alkalmas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zIApR8YqVOM"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}